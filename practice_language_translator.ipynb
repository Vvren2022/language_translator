{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrUrhOuYO9p7",
        "outputId": "065a5bbd-873d-4de8-a3ba-b4b54e839f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.69.0)\n",
            "Collecting openai\n",
            "  Downloading openai-1.70.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Downloading openai-1.70.0-py3-none-any.whl (599 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m599.1/599.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.69.0\n",
            "    Uninstalling openai-1.69.0:\n",
            "      Successfully uninstalled openai-1.69.0\n",
            "Successfully installed openai-1.70.0\n"
          ]
        }
      ],
      "source": [
        "#  !pip install langchain\n",
        "# !pip install langchain openai\n",
        "# !pip install faiss-cpu\n",
        "# !pip install nltk\n",
        "!pip install --upgrade openai\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain-community"
      ],
      "metadata": {
        "id": "8uXSDKOUSbGR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "import os\n",
        "import openai\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "2GXfBKVwTXS8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]=grisha_key\n"
      ],
      "metadata": {
        "id": "3r1w7h9VSfQi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.1)\n"
      ],
      "metadata": {
        "id": "CjlHm_k2TC4R"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import numpy as np\n",
        "\n",
        "import faiss\n",
        "\n",
        "# Function to get embeddings for a text\n",
        "def get_embedding(text):\n",
        "    response = openai.Embedding.create(\n",
        "        input=text,\n",
        "        model=\"text-embedding-ada-002\"  # OpenAI's embedding model\n",
        "    )\n",
        "    return np.array(response['data'][0]['embedding'], dtype=np.float32)\n",
        "\n",
        "\n",
        "# Initialize FAISS index (L2 distance for similarity search)\n",
        "d = 1536  # Dimension of the embeddings (OpenAI's ada embeddings)\n",
        "index = faiss.IndexFlatL2(d)  # This is a simple index for demonstration\n",
        "\n",
        "# Store translations in a dictionary (to fetch translations later)\n",
        "db = {}\n",
        "\n",
        "# Function to add text to the FAISS database\n",
        "def add_to_db(text, translation):\n",
        "    embedding = get_embedding(text)\n",
        "    index.add(embedding.reshape(1, -1))  # Add embedding to FAISS index\n",
        "    db[index.ntotal - 1] = translation  # Save translation against index id\n",
        "\n",
        "\n",
        "# Function to perform translation\n",
        "def translate_text(text, target_language):\n",
        "    # Get embedding for input text\n",
        "    embedding = get_embedding(text)\n",
        "\n",
        "    # Search for similar translations in FAISS\n",
        "    if index.ntotal > 0:\n",
        "        D, I = index.search(embedding.reshape(1, -1), 1)  # Search for nearest translation\n",
        "        if D[0][0] < 0.1:  # Threshold for similarity\n",
        "            return db[I[0][0]]  # If similar, return the translation\n",
        "\n",
        "    # Otherwise, use GPT-4 for translation\n",
        "    prompt = f\"Translate this text into {target_language}: {text}\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant that provides translations.\"\n",
        "        }, {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }]\n",
        "    )\n",
        "    translation = response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "    # Store the new translation in FAISS and DB\n",
        "    index.add(embedding.reshape(1, -1))  # Add to FAISS index\n",
        "    db[index.ntotal - 1] = translation  # Save translation\n",
        "\n",
        "    return translation\n"
      ],
      "metadata": {
        "id": "Nov47cweTqfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import numpy as np\n",
        "\n",
        "# Function to get embeddings for a text\n",
        "def get_embedding(text):\n",
        "    response = openai.Embedding.create(\n",
        "        input=text,\n",
        "        model=\"text-embedding-ada-002\"  # OpenAI's embedding model\n",
        "    )\n",
        "    return np.array(response['data'][0]['embedding'], dtype=np.float32)\n",
        "\n",
        "\n",
        "get_embedding(\"ich bin viren \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "wWik8Lx2VZi9",
        "outputId": "7d92741e-d767-4217-bc9d-9c6d428552ad"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "APIRemovedInV1",
          "evalue": "\n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-649b652e4e50>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ich bin viren \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-649b652e4e50>\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Function to get embeddings for a text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     response = openai.Embedding.create(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-embedding-ada-002\"\u001b[0m  \u001b[0;31m# OpenAI's embedding model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/lib/_old_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAPIRemovedInV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "# Function to get embedding for a single text\n",
        "def get_embedding(text):\n",
        "    # Use embed_documents for embedding the text\n",
        "    embeddings = embedding_model.embed_documents([text])\n",
        "    return embeddings[0]  # Return the first embedding\n"
      ],
      "metadata": {
        "id": "ArX16nksVcIW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "opaDXkWyWDo2"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create FAISS Index (flat L2 distance index)\n",
        "d = 1536  # Dimension of the embedding vectors (text-embedding-ada-002 has 1536 dimensions)\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "# Store translations in a dictionary (for quick lookup)\n",
        "db = {}\n",
        "\n",
        "# Function to add text and its translation to FAISS and db\n",
        "def add_to_db(text, translation):\n",
        "    embedding = get_embedding(text)\n",
        "    index.add(np.array(embedding).reshape(1, -1))  # Add embedding to FAISS\n",
        "    db[index.ntotal - 1] = translation  # Store translation in dictionary\n",
        "\n"
      ],
      "metadata": {
        "id": "FXUJ6IC7WvRc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_text(text, target_language):\n",
        "    # Get the embedding for the input text\n",
        "    embedding = get_embedding(text)\n",
        "\n",
        "    # Search the FAISS index for similar translations\n",
        "    if index.ntotal > 0:\n",
        "        D, I = index.search(np.array(embedding).reshape(1, -1), 1)  # Search for nearest match\n",
        "        if D[0][0] < 0.1:  # Threshold for similarity (you can adjust this)\n",
        "            return db[I[0][0]]  # Return the translation if a match is found\n",
        "\n",
        "    # Use GPT-4 to translate the text if no match is found\n",
        "    prompt = f\"Translate the following text to {target_language}: {text}\"\n",
        "\n",
        "    # Generate translation using OpenAI's LLM (via LangChain)\n",
        "    response = llm.invoke([{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant providing translations.\"\n",
        "    }, {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }])\n",
        "\n",
        "    # Access the content of the response (message content)\n",
        "    translation = response.content.strip()  # Get the content of the first message\n",
        "\n",
        "    # Store the new translation in the FAISS index and dictionary\n",
        "    index.add(np.array(embedding).reshape(1, -1))  # Add new embedding to FAISS index\n",
        "    db[index.ntotal - 1] = translation  # Save translation to the dictionary\n",
        "\n",
        "    return translation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TN3ZVFOsW-FV"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_text(\"ich bin viren\", \"english\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uYAcXexrX4hN",
        "outputId": "61056df1-8dc7-4c90-db19-6ed3fe09bbe1"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The English translation is: I am viruses.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tiktoken"
      ],
      "metadata": {
        "id": "0109YCs3YNMH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# Initialize OpenAI LLM (Large Language Model)\n",
        "llm = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "# Function to generate translation\n",
        "def translate_text(text, target_language):\n",
        "    # Construct the prompt to translate the text\n",
        "    prompt = f\"Translate the following text to {target_language}: {text}\"\n",
        "\n",
        "    # Generate translation using OpenAI's LLM (via LangChain)\n",
        "    response = llm.invoke([{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant providing translations.\"\n",
        "    }, {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }])\n",
        "\n",
        "    # Extract the translation from the response\n",
        "    translation = response.content.strip()  # Get the content of the first message\n",
        "\n",
        "    return translation\n"
      ],
      "metadata": {
        "id": "bgxEODwzY_Z8"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_text(\"i am viren vaviya\",\"german\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IORbB_V1a7ui",
        "outputId": "aed39d5d-9376-4e64-a120-ceb5feb6b0c1"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ich bin Viren Vaviya'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_language = \"german\"\n",
        "text = \"i am viren\"\n",
        "\n",
        "# Construct the prompt to translate the text\n",
        "prompt = f\"Translate the following text to {target_language}: {text}\"\n",
        "\n",
        "    # Generate translation using OpenAI's LLM (via LangChain)\n",
        "response = llm.invoke([{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant providing translations.\"\n",
        "    }, {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }])\n",
        "\n",
        "translation=response.content.strip()"
      ],
      "metadata": {
        "id": "7m8Cl94GbW5p"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "cleaned_translation = re.sub(r'[^a-zA-Z\\s]', '', translation)\n",
        "# matches = re.findall(r'\\b\\w+\\b', translation)"
      ],
      "metadata": {
        "id": "UTF6pfaJbpQp"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Translation:\", translation)\n",
        "print(\"Cleaned Translation:\", cleaned_translation)\n",
        "print(\"Matches found with regex:\", matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w27zYr484hj_",
        "outputId": "5bb348ed-2419-434f-d27c-8c36461412e5"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Translation: Ich bin Viren.\n",
            "Cleaned Translation: Ich bin Viren\n",
            "Matches found with regex: ['Ich', 'bin', 'Viren']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.2, max_tokens=2000)\n",
        "\n",
        "# Function to split text into chunks\n",
        "def split_text(text, max_words=500):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)  # Split at sentence boundaries\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    word_count = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        if word_count + len(words) > max_words:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "            word_count = 0\n",
        "        current_chunk.append(sentence)\n",
        "        word_count += len(words)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to translate text chunk-by-chunk\n",
        "def translate_large_text(text, target_language):\n",
        "    chunks = split_text(text)\n",
        "    translated_chunks = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        prompt = f\"Translate the following text to {target_language}: {chunk}\"\n",
        "        response = llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
        "        translated_text = response.content.strip()\n",
        "        translated_chunks.append(translated_text)\n",
        "\n",
        "    return \" \".join(translated_chunks)\n",
        "\n",
        "# Example Usage\n",
        "input_text = \"ich bin viren\"\n",
        "target_language = \"English\"\n",
        "\n",
        "translated_text = translate_large_text(input_text, target_language)\n",
        "print(\"Translated Text:\", translated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFnnXLaR4tAV",
        "outputId": "5c303163-7289-40ce-d11b-1c9aed3bd2b3"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Text: The translation of \"ich bin viren\" from German to English is \"I am viruses.\" However, this seems like an unusual statement. If you meant to say \"I have viruses,\" the correct German would be \"ich habe Viren.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_translation_prompt(text, target_language):\n",
        "    return f\"\"\"\n",
        "    You are a professional translator.\n",
        "    Translate the following text **accurately** into {target_language} while preserving meaning, tone, and style.\n",
        "    If the text contains idioms, slang, or culturally specific references, translate them to an **equivalent** phrase in {target_language}.\n",
        "    Ensure proper grammar, sentence structure, and fluency in {target_language}.\n",
        "\n",
        "    Text:\n",
        "    \"{text}\"\n",
        "    \"\"\"\n",
        "\n",
        "# Example usage\n",
        "text = \"It's raining cats and dogs.\"\n",
        "target_language = \"German\"\n",
        "prompt = get_translation_prompt(text, target_language)\n",
        "print(prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4Xi5e8-_pNk",
        "outputId": "82b88d8d-0cbe-4c7a-91ba-9cf7c625ebf7"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    You are a professional translator.\n",
            "    Translate the following text **accurately** into German while preserving meaning, tone, and style.\n",
            "    If the text contains idioms, slang, or culturally specific references, translate them to an **equivalent** phrase in German.\n",
            "    Ensure proper grammar, sentence structure, and fluency in German.\n",
            "\n",
            "    Text:\n",
            "    \"It's raining cats and dogs.\"\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.2, max_tokens=2000)\n",
        "\n",
        "# Function to generate translation prompt\n",
        "def get_translation_prompt(text, target_language):\n",
        "    return f\"\"\"\n",
        "    You are a professional translator.\n",
        "    Translate the following text **accurately** into {target_language} while preserving meaning, tone, and style.\n",
        "    If the text contains idioms, slang, or culturally specific references, translate them to an **equivalent** phrase in {target_language}.\n",
        "    Ensure proper grammar, sentence structure, and fluency in {target_language}.\n",
        "\n",
        "    Text:\n",
        "    \"{text}\"\n",
        "    \"\"\"\n",
        "\n",
        "# Function to get translation from OpenAI\n",
        "def translate_text(text, target_language):\n",
        "    prompt = get_translation_prompt(text, target_language)\n",
        "\n",
        "    response = llm.invoke([\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ])\n",
        "\n",
        "    return response.content.strip()  # Extracting the response content\n",
        "\n",
        "# Example usage\n",
        "text = \"It's raining cats and dogs.\"\n",
        "target_language = \"German\"\n",
        "translation = translate_text(text, target_language)\n",
        "\n",
        "print(\"Translation:\", translation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LdUTJtTADEh",
        "outputId": "fa2f3770-9f5d-48af-ad36-ad5a5d5e065c"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation: Es regnet BindfÃ¤den.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.2, max_tokens=2000)\n",
        "\n",
        "# Function to generate translation prompt\n",
        "def get_translation_prompt(text, target_language):\n",
        "    return f\"\"\"\n",
        "    You are a professional translator.\n",
        "    Translate the following text into {target_language} while preserving its true meaning, tone, and **cultural context**.\n",
        "    Do not translate idioms, slang, or culturally specific phrases literally. Instead, use an equivalent expression in {target_language}.\n",
        "    Maintain **formal grammar** and **natural fluency**.\n",
        "\n",
        "    Text: \"{text}\"\n",
        "    \"\"\"\n",
        "\n",
        "# Function to get translation from OpenAI\n",
        "def translate_text(text, target_language):\n",
        "    prompt = get_translation_prompt(text, target_language)\n",
        "\n",
        "    response = llm.invoke([\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ])\n",
        "\n",
        "    return response.content.strip()  # Extracting the response content\n",
        "\n",
        "# Example usage\n",
        "# text = \"It's raining cats and dogs.\"  # English idiom\n",
        "text=\"ich bin viren\"\n",
        "target_language = \"English\"\n",
        "translation = translate_text(text, target_language)\n",
        "\n",
        "print(\"Translation:\", translation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikF7Qr5MBOPU",
        "outputId": "5d8364e6-3afa-45b7-c2fc-97c433dfaddd"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation: I am ill.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.2, max_tokens=2000)\n",
        "\n",
        "# Function to generate translation prompt\n",
        "def get_translation_prompt(text, target_language):\n",
        "    return f\"\"\"\n",
        "    You are a professional translator.\n",
        "    Translate the following text into {target_language} while maintaining **accuracy, fluency, and natural tone**.\n",
        "    - **Simple words** should be translated **directly** without adding unnecessary complexity.\n",
        "    - **Idioms, slang, and cultural references** should be translated using an **equivalent expression** in {target_language}, not word-for-word.\n",
        "    - Maintain **formal grammar** where appropriate.\n",
        "\n",
        "    Text: \"{text}\"\n",
        "    \"\"\"\n",
        "\n",
        "# Function to get translation from OpenAI\n",
        "def translate_text(text, target_language):\n",
        "    prompt = get_translation_prompt(text, target_language)\n",
        "\n",
        "    response = llm.invoke([\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ])\n",
        "\n",
        "    return response.content.strip()  # Extracting the response content\n",
        "\n",
        "# Example usage\n",
        "# text = \"Hello, my name is Viren. It's raining cats and dogs.\"\n",
        "text=\"i am viren vaviya\"\n",
        "target_language = \"English\"\n",
        "translation = translate_text(text, target_language)\n",
        "\n",
        "print(\"Translation:\", translation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNWA0YFQBtB-",
        "outputId": "01f50134-0422-4b20-92c0-43643d888f4c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation: My name is Viren Vaviya.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.2, max_tokens=2000)\n",
        "\n",
        "# Function to generate translation prompt\n",
        "def get_translation_prompt(text, target_language):\n",
        "    return f\"\"\"\n",
        "    You are a professional translator.\n",
        "    Translate the following text into {target_language} while maintaining **accuracy, fluency, and natural tone**.\n",
        "    - **Simple words** should be translated **directly** without adding unnecessary complexity.\n",
        "    - **Idioms, slang, and cultural references** should be translated using an **equivalent expression** in {target_language}, not word-for-word.\n",
        "    - Maintain **formal grammar** where appropriate.\n",
        "\n",
        "    Text: \"{text}\"\n",
        "    \"\"\"\n",
        "\n",
        "# Function to translate text into any language\n",
        "def translate_text(text, target_language):\n",
        "    prompt = get_translation_prompt(text, target_language)\n",
        "\n",
        "    response = llm.invoke([\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ])\n",
        "\n",
        "    return response.content.strip()  # Extract only the translated text\n",
        "\n",
        "# Example usage: Translate to different languages\n",
        "text = \"Hello, my name is Viren. It's raining cats and dogs.\"\n",
        "\n",
        "languages = [\"French\", \"Spanish\", \"Chinese\", \"Hindi\", \"Arabic\", \"Russian\", \"Japanese\", \"Italian\"]\n",
        "\n",
        "# for lang in languages:\n",
        "#     translation = translate_text(text, lang)\n",
        "#     print(f\"\\nðŸ”¹ Translation in {lang}: {translation}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqzZ_kZ-CJz3",
        "outputId": "e8fa6b50-ca58-4757-87b4-8b45b8f9b8c0"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¹ Translation in French: Bonjour, je m'appelle Viren. Il pleut des cordes.\n",
            "\n",
            "ðŸ”¹ Translation in Spanish: Hola, mi nombre es Viren. EstÃ¡ lloviendo a cÃ¡ntaros.\n",
            "\n",
            "ðŸ”¹ Translation in Chinese: ä½ å¥½ï¼Œæˆ‘å«ç»´ä¼¦ã€‚çŽ°åœ¨æ­£ä¸‹ç€å€¾ç›†å¤§é›¨ã€‚\n",
            "\n",
            "ðŸ”¹ Translation in Hindi: à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤®à¥‡à¤°à¤¾ à¤¨à¤¾à¤® à¤µà¤¿à¤°à¥‡à¤¨ à¤¹à¥ˆà¥¤ à¤¬à¤¹à¥à¤¤ à¤¤à¥‡à¤œà¤¼ à¤¬à¤¾à¤°à¤¿à¤¶ à¤¹à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤\n",
            "\n",
            "ðŸ”¹ Translation in Arabic: Ù…Ø±Ø­Ø¨Ù‹Ø§ØŒ Ø§Ø³Ù…ÙŠ ÙÙŠØ±ÙŠÙ†. Ø¥Ù†Ù‡Ø§ ØªÙ…Ø·Ø± Ø¨ØºØ²Ø§Ø±Ø©.\n",
            "\n",
            "ðŸ”¹ Translation in Russian: ÐŸÑ€Ð¸Ð²ÐµÑ‚, Ð¼ÐµÐ½Ñ Ð·Ð¾Ð²ÑƒÑ‚ Ð’Ð¸Ñ€ÐµÐ½. Ð›ÑŒÐµÑ‚ ÐºÐ°Ðº Ð¸Ð· Ð²ÐµÐ´Ñ€Ð°.\n",
            "\n",
            "ðŸ”¹ Translation in Japanese: ã“ã‚“ã«ã¡ã¯ã€ç§ã®åå‰ã¯ãƒ´ã‚£ãƒ¬ãƒ³ã§ã™ã€‚åœŸç ‚é™ã‚Šã§ã™ã€‚\n",
            "\n",
            "ðŸ”¹ Translation in Italian: Ciao, mi chiamo Viren. Sta piovendo a catinelle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# language_helper.py\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import math\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.2, max_tokens=2000)\n",
        "\n",
        "# Function to generate translation prompt\n",
        "def get_translation_prompt(text, target_language):\n",
        "    return f\"\"\"\n",
        "    You are a professional translator.\n",
        "    Translate the following text into {target_language} while maintaining **accuracy, fluency, and natural tone**.\n",
        "    - **Simple words** should be translated **directly** without adding unnecessary complexity.\n",
        "    - **Idioms, slang, and cultural references** should be translated using an **equivalent expression** in {target_language}, not word-for-word.\n",
        "    - Maintain **formal grammar** where appropriate.\n",
        "\n",
        "    Text: \"{text}\"\n",
        "    \"\"\"\n",
        "\n",
        "# Function to chunk text if it's too long\n",
        "def chunk_text(text, max_length=2000):\n",
        "    # Split text into chunks that fit the model's max token limit\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    chunk = []\n",
        "\n",
        "    for word in words:\n",
        "        chunk.append(word)\n",
        "        if len(' '.join(chunk)) > max_length:\n",
        "            chunks.append(' '.join(chunk[:-1]))  # Add the previous chunk\n",
        "            chunk = [word]  # Start new chunk\n",
        "\n",
        "    if chunk:\n",
        "        chunks.append(' '.join(chunk))  # Add the last chunk\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to translate text into any language with chunking for long text\n",
        "def translate_text(text, target_language):\n",
        "    prompt = get_translation_prompt(text, target_language)\n",
        "\n",
        "    # If the text is too long, split it into smaller chunks\n",
        "    chunks = chunk_text(text)\n",
        "\n",
        "    translation = \"\"\n",
        "    for chunk in chunks:\n",
        "        # Generate the translation for each chunk\n",
        "        response = llm.invoke([\n",
        "            {\"role\": \"user\", \"content\": prompt + \"\\n\" + chunk}\n",
        "        ])\n",
        "        translation += response.content.strip() + \" \"  # Combine translated chunks\n",
        "\n",
        "    return translation.strip()  # Return full translated text after combining chunks\n"
      ],
      "metadata": {
        "id": "QEWq1TtoCou_"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_text(\"i am viren vaviya\",\"French\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bQm-bAsXFTIQ",
        "outputId": "168ed0c1-9b5e-4e54-bb5f-79d66e2e4b7a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Je suis Viren Vaviya.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# language_helper.py\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import tiktoken\n",
        "\n",
        "# Initialize the OpenAI LLM (assuming GPT-4 or GPT-4 Turbo)\n",
        "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.2, max_tokens=2000)\n",
        "\n",
        "# Function to generate translation prompt\n",
        "def get_translation_prompt(text, target_language):\n",
        "    return f\"\"\"\n",
        "    You are a professional translator.\n",
        "    Translate the following text into {target_language} while maintaining **accuracy, fluency, and natural tone**.\n",
        "    - **Simple words** should be translated **directly** without adding unnecessary complexity.\n",
        "    - **Idioms, slang, and cultural references** should be translated using an **equivalent expression** in {target_language}, not word-for-word.\n",
        "    - Maintain **formal grammar** where appropriate.\n",
        "\n",
        "    Text: \"{text}\"\n",
        "    \"\"\"\n",
        "\n",
        "# Function to calculate the number of tokens in a text\n",
        "def count_tokens(text):\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")  # Use the appropriate encoding for the model\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "# Function to chunk text if it's too long\n",
        "def chunk_text(text, max_tokens=2000):\n",
        "    # Split text into chunks that fit the model's max token limit\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    chunk = []\n",
        "    current_tokens = 0\n",
        "\n",
        "    for word in words:\n",
        "        chunk.append(word)\n",
        "        current_tokens = count_tokens(' '.join(chunk))\n",
        "        if current_tokens > max_tokens:\n",
        "            chunks.append(' '.join(chunk[:-1]))  # Add the previous chunk\n",
        "            chunk = [word]  # Start new chunk\n",
        "            current_tokens = count_tokens(word)\n",
        "\n",
        "    if chunk:\n",
        "        chunks.append(' '.join(chunk))  # Add the last chunk\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to translate text into any language with chunking for long text\n",
        "def translate_text(text, target_language):\n",
        "    prompt = get_translation_prompt(text, target_language)\n",
        "\n",
        "    # If the text is too long, split it into smaller chunks\n",
        "    chunks = chunk_text(text)\n",
        "\n",
        "    translation = \"\"\n",
        "    for chunk in chunks:\n",
        "        # Generate the translation for each chunk\n",
        "        try:\n",
        "            response = llm.invoke([  # Use llm.call() or .invoke() depending on the setup\n",
        "                {\"role\": \"user\", \"content\": prompt + \"\\n\" + chunk}\n",
        "            ])\n",
        "            translation += response.content.strip() + \" \"  # Combine translated chunks\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating chunk: {e}\")\n",
        "            return None  # Handle error appropriately\n",
        "\n",
        "    return translation.strip()  # Return full translated text after combining chunks\n"
      ],
      "metadata": {
        "id": "A4RPO45lFcnR"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_text(\"viren vaviya in germany\",\"German\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kGq5hwWBHsWo",
        "outputId": "aeca07d7-7a0a-4ca4-f6ab-2028e3aa6546"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Viren Vaviya in Deutschland'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oya5vOKyHzh3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}